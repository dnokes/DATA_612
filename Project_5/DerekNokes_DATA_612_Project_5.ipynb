{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 612 - Project 5\n",
    "**Derek G. Nokes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we create a simple but scalable recommender system using PySpark and compare it to a similar recommender system created for Project 3. \n",
    "\n",
    "In previous projects for this course, we created our own functions to download the movielens dataset [@MovieLens]. We re-use these functions to acquire the required data.\n",
    "\n",
    "In the first section, we provide instructions for replicating our computing environment using a custom docker image. We review the data acquisition in the second section. In the third section, we demonstrate how PySpark can be used to explore data, then in the fourth section we provide an overview of the theory underlying the two models we implement and compare in the fifth and sixth sections respectively. In the final section we provide our concluding remarks.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Environment\n",
    "\n",
    "To ensure that our work can easily be replicated, we have created a docker container with all of the required tools installed. In this section, we provide instructions for acquiring and running this project in a docker container.\n",
    "\n",
    "To create our environment we simply pulled jupyter/all-spark-notebook:latest image and added the recommender systems package, 'surprise'.\n",
    "\n",
    "### Creating the Custom Docker Image\n",
    "\n",
    "One a system with docker installed, at the command line run:\n",
    "\n",
    "docker pull jupyter/all-spark-notebook:latest\n",
    "\n",
    "Start the contain once it has downloaded:\n",
    "\n",
    "docker run -it -p 8888:8888 -v /home/dnokes/projects/ms/github/DATA_612/Project_5:/home/dnokes/projects/ms/github/DATA_612/Project_5 jupyter/all-spark-notebook:latest\n",
    "\n",
    "The first path is the local path. The second path is the remote path inside the container\n",
    "\n",
    "Open another terminal window and at the command line type\n",
    "\n",
    "docker ps \n",
    "\n",
    "Locate the container ID (e.g. , 0d78bfb39601). \n",
    "\n",
    "Open another terminal window and run the command\n",
    "\n",
    "docker exec -it 0d78bfb39601 /bin/bash\n",
    "\n",
    "Add the 'surprise' package. Type:\n",
    "\n",
    "conda install -c conda-forge scikit-surprise\n",
    "\n",
    "Click 'Y' when prompted.\n",
    "\n",
    "Go back to the terminal window where we ran docker ps.\n",
    "\n",
    "Now we create the custom docker image. Type:\n",
    "\n",
    "docker commit 0d78bfb39601 dgn2/data_612_project_5:version1\n",
    "\n",
    "Finally, we push the image to dockerhub:\n",
    "\n",
    "### Running the Custom Docker Image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd \n",
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "# import MF library\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "#from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# create sparksession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pysparkexample\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(spark)\n",
    "# check if spark context is defined\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition\n",
    "\n",
    "In this section we download the movielens data and read the data into a Spark dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to download the movielens data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download movielens data\n",
    "def download(download_url, download_path):\n",
    "\n",
    "    req = requests.get(download_url, stream=True)\n",
    "\n",
    "    with open(download_path, 'wb') as fd:\n",
    "        for chunk in req.iter_content(chunk_size=2**20):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define download path\n",
    "#download_path='/home/dnokes/projects/ms/github/DATA_612/Project_5/ml-100k.zip'\n",
    "download_path='ml-100k.zip'\n",
    "# define download URL\n",
    "download_url='http://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
    "# dowload movielens data\n",
    "download(download_url, download_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the data from the zip file and read it into memory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define unzip path\n",
    "unzip_path=''\n",
    "# unzip movielens data\n",
    "zf = zipfile.ZipFile(download_path)\n",
    "user_by_item_file=zf.extract('ml-100k/u.data', unzip_path)\n",
    "title_by_item_file=zf.extract('ml-100k/u.item', unzip_path)\n",
    "\n",
    "# read movielens data into dataframe\n",
    "df_user_by_item = pd.read_csv(user_by_item_file, sep='\\t',header=None,\n",
    "    names=['user_id','item_id','rating','titmestamp'])\n",
    "df_title_by_item = pd.read_csv(title_by_item_file, sep='|',encoding='latin-1',\n",
    "    header=None,usecols=[0,1],names=['item_id', 'title'])\n",
    "# join the ratings by user and item to the title\n",
    "df = pd.merge(df_title_by_item,df_user_by_item, on='item_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the dimensions of the component and merged datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_title_by_item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_by_item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 5)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert our Pandas dataframe to a Spark dataframe for use in the next sections of the project. While this approach works for our small dataset, obviously we would read the data directly into Spark if we were using a much larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sql_sc = SQLContext(spark)\n",
    "# convert to spark\n",
    "s_df = sql_sc.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "In this section, we quickly explore the Spark functionality for manipulating and exploring data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we show the top 5 lines of the pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>titmestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>308</td>\n",
       "      <td>4</td>\n",
       "      <td>887736532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>287</td>\n",
       "      <td>5</td>\n",
       "      <td>875334088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>148</td>\n",
       "      <td>4</td>\n",
       "      <td>877019411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>280</td>\n",
       "      <td>4</td>\n",
       "      <td>891700426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>883601324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id             title  user_id  rating  titmestamp\n",
       "0        1  Toy Story (1995)      308       4   887736532\n",
       "1        1  Toy Story (1995)      287       5   875334088\n",
       "2        1  Toy Story (1995)      148       4   877019411\n",
       "3        1  Toy Story (1995)      280       4   891700426\n",
       "4        1  Toy Story (1995)       66       3   883601324"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display top 5 lines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we show the top 5 lines of the Spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_id=1, title='Toy Story (1995)', user_id=308, rating=4, titmestamp=887736532),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=287, rating=5, titmestamp=875334088),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=148, rating=4, titmestamp=877019411),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=280, rating=4, titmestamp=891700426),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=66, rating=3, titmestamp=883601324)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the schema as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- item_id: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- titmestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Spark we can summarize the data quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+-----------------+------------------+-----------------+\n",
      "|summary|           item_id|               title|          user_id|            rating|       titmestamp|\n",
      "+-------+------------------+--------------------+-----------------+------------------+-----------------+\n",
      "|  count|            100000|              100000|           100000|            100000|           100000|\n",
      "|   mean|         425.53013|                null|        462.48475|           3.52986|8.8352885148862E8|\n",
      "| stddev|330.79835632558525|                null|266.6144201275087|1.1256735991443156|5343856.189502792|\n",
      "|    min|                 1|'Til There Was Yo...|                1|                 1|        874724710|\n",
      "|    max|              1682|Á köldum klaka (C...|              943|                 5|        893286638|\n",
      "+-------+------------------+--------------------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the mean rating by user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|user_id|       avg(rating)|\n",
      "+-------+------------------+\n",
      "|     26|  2.94392523364486|\n",
      "|    474|  4.08256880733945|\n",
      "|     29|3.6470588235294117|\n",
      "|    541|3.6240601503759398|\n",
      "|     65|            3.9375|\n",
      "|    558|               4.2|\n",
      "|    191|3.6296296296296298|\n",
      "|    418|               2.9|\n",
      "|    222| 3.049095607235142|\n",
      "|    293|3.0309278350515463|\n",
      "|    730| 3.236842105263158|\n",
      "|    938|3.2685185185185186|\n",
      "|    270| 4.333333333333333|\n",
      "|    243|3.6419753086419755|\n",
      "|    705| 3.710526315789474|\n",
      "|    442| 3.125874125874126|\n",
      "|    367| 4.155172413793103|\n",
      "|    278| 4.260869565217392|\n",
      "|    720| 3.966666666666667|\n",
      "|     54|3.6923076923076925|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_df.groupby('user_id').agg({'rating': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the mean rating by item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|item_id|       avg(rating)|\n",
      "+-------+------------------+\n",
      "|     26| 3.452054794520548|\n",
      "|     29|2.6666666666666665|\n",
      "|    474| 4.252577319587629|\n",
      "|    964|3.3333333333333335|\n",
      "|   1677|               3.0|\n",
      "|     65|3.5391304347826087|\n",
      "|    191| 4.163043478260869|\n",
      "|    418|3.5813953488372094|\n",
      "|    541| 2.877551020408163|\n",
      "|    558|3.6714285714285713|\n",
      "|   1010|              3.25|\n",
      "|   1224|2.6666666666666665|\n",
      "|   1258|2.5217391304347827|\n",
      "|   1277|3.4210526315789473|\n",
      "|   1360|               1.5|\n",
      "|    222|  3.66027397260274|\n",
      "|    270|3.5955882352941178|\n",
      "|    293| 3.802721088435374|\n",
      "|    730|               3.5|\n",
      "|    938|              2.88|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_df.groupby('item_id').agg({'rating': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the number of ratings by user id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+---+\n",
      "|user_id_rating|  1|  2|  3|  4|  5|\n",
      "+--------------+---+---+---+---+---+\n",
      "|           645|  2|  2| 29| 55| 34|\n",
      "|           892|  2| 13| 40| 99| 72|\n",
      "|            69|  2|  3| 21| 16| 23|\n",
      "|           809|  2|  2|  6|  5|  5|\n",
      "|           629|  1|  8| 24| 35| 53|\n",
      "|           365|  5|  9| 12| 23|  9|\n",
      "|           138|  0|  1|  3| 28| 19|\n",
      "|           760|  4|  7| 11| 13|  6|\n",
      "|           101|  3| 19| 28| 16|  1|\n",
      "|           479| 24| 14| 48| 85| 31|\n",
      "|           347| 20| 25| 37| 55| 62|\n",
      "|           846| 10| 46| 89|154|106|\n",
      "|           909|  0|  0|  5|  7| 14|\n",
      "|           333|  1|  1|  5| 13|  6|\n",
      "|           628|  0|  1|  1|  3| 22|\n",
      "|           249|  1|  5| 31| 63| 61|\n",
      "|           893|  1|  5| 28| 18|  7|\n",
      "|           518|  4|  3| 28| 18| 20|\n",
      "|           468|  0|  9| 31| 55| 48|\n",
      "|           234| 14|103|205|126| 32|\n",
      "+--------------+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_df.crosstab('user_id', 'rating').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the number of ratings by item id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+---+---+---+---+\n",
      "|item_id_rating|  1|  2|  3|  4|  5|\n",
      "+--------------+---+---+---+---+---+\n",
      "|           645|  1|  0|  9| 10|  7|\n",
      "|           892|  7| 13| 22|  8|  3|\n",
      "|            69| 14| 23| 59|125|100|\n",
      "|          1322|  1|  2|  3|  0|  0|\n",
      "|          1665|  0|  1|  0|  0|  0|\n",
      "|          1036|  2| 12|  2|  6|  2|\n",
      "|          1586|  1|  0|  0|  0|  0|\n",
      "|          1501|  1|  0|  2|  2|  0|\n",
      "|           809|  4| 10| 16| 12|  1|\n",
      "|          1337|  3|  0|  6|  0|  0|\n",
      "|          1411|  6|  7|  8|  7|  0|\n",
      "|           629|  3|  8| 33| 26|  7|\n",
      "|          1024|  1|  1|  6|  2|  5|\n",
      "|          1469|  2|  0|  5|  4|  1|\n",
      "|           365|  3| 10| 16| 12|  7|\n",
      "|          1369|  1|  0|  2|  1|  0|\n",
      "|           138|  6|  3|  5|  3|  2|\n",
      "|          1190|  0|  2|  5|  2|  2|\n",
      "|          1168|  1|  4|  6|  9|  2|\n",
      "|           760| 11|  8| 18|  4|  5|\n",
      "+--------------+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s_df.crosstab('item_id', 'rating').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SQL to run queries against our dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reguster dataset as table\n",
    "s_df.createOrReplaceTempView('movielens')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of ratings for each distinct rating level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|rating|nRatings|\n",
      "+------+--------+\n",
      "|     1|    6110|\n",
      "|     2|   11370|\n",
      "|     3|   27145|\n",
      "|     4|   34174|\n",
      "|     5|   21201|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select rating,count(rating) as nRatings from movielens group by rating order by rating').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have familarized ourselves with some of the functionality of Spark for exploring data, we very breifly review the theory underlying the recommender model we will implement in later sections of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "In Project 3, we used Probabilistic Matrix Factorization to create a recommender system. In the first sub-section of the 'Theory' section we review the underlying theory for Probabilistic Matrix Factorization, then we in the second sub-section we provide an overview of the Spark implementation.\n",
    "\n",
    "### Probabilistic Matrix Factorization Using Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Probabilistic matrix factorization [@Salakhutdinov-2008a] is very similar to singular value decomposition. We define the probabilistic matrix factorization as follows: \n",
    "\n",
    "$$R = Q^{T}P$$\n",
    "\n",
    "where predictions $\\hat{r}_{ui}$ for user $u$ for item $i$,  are:\n",
    "\n",
    "$$\\hat{r}_{ui} = q_i^{T}p_u$$\n",
    "\n",
    "where $q_i$ and $p_u$ are the latent factors for items and users respectively.\n",
    "\n",
    "Unlike the classical SVD, this matrix factorization can be applied to sparse matrices. We minimize the regularized squared error using stochastic gradient descent to estimate all of the unknowns as follows:\n",
    "\n",
    "$$\\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 + \\lambda\\left(\\big\\|q_i\\big\\|^{2} + \\big\\|p_u\\big\\|^{2}\\right)$$\n",
    "\n",
    "where $r_{ui} - \\hat{r}_{ui}$ is the error and $\\lambda$ is the regularization parameter.\n",
    "\n",
    "### Probabilistic Matrix Factorization Using Alternating Least Squares (ALS) With Weighted-$\\lambda$-Regularization\n",
    "\n",
    "As the rating matrix contains both signals and noise, it is important to remove noise and use the recovered signal to predict missing ratings. Singular Value Decomposition (SVD) is a natural approach that approximates the original user-movie rating matrix $R$ by the product of two rank-$k$ matrices of $\\hat{R} = U^{T} \\times M$. The solution given by the SVD minimizes the Frobenious norm of $R - \\hat{R}$, which is equivalent to minimizing the RMSE over all elements of $R$, stand SVD algorithms cannot find $U$ and $M$.\n",
    "\n",
    "In this project, we use the *alternating least squares* (ALS) algorithm to solve the low rank matrix factorization problem using the following steps:\n",
    "\n",
    "**Step 1**: Initialize matrix $M$ by assigning the average rating for that movie as the first row, and small random numbers for the remaining entries.\n",
    "\n",
    "**Step 2**: Fix $M$, Solve $U$ by minimizing the objective function (the sum of squared errors);\n",
    "\n",
    "**Step 3**: Fix $U$, solve $M$ by minimizing the objective function similarly;\n",
    "\n",
    "**Step 4**: Repeat Steps 2 and 3 until a stopping criterion is satisfied.\n",
    "\n",
    "The stopping criterion used is based on the observed root mean squared error (RMSE) on the training dataset. After one round of updating both $U$ and $M$, if the difference between the observed RMSEs on the training dataset is less than 0.0001, the iteration stops and we use the obtained $U,M$ to make final predictions on the testing dataset. The same objective function was used previously by Salakhutdinov et al. and solved using gradient descent. [In fact, in Project 3, we used this approach? We will compare the results of ALS and SGD]] \n",
    "\n",
    "Without regularization, ALS might lead to overfitting as there are many free parameters. A common fix is to use Tikhonov regularization, which penalizes large parameters. \n",
    "\n",
    "The following weighted-$\\lambda$-regularization has been found to prevent overfitting as the number of features or number of iterations increase:\n",
    "\n",
    "$$f(U,M)=\\sum_{(i,j) \\in I}\\big(r_{i,j}-u_{i}^{T}m_{j}\\big)^{2}+\\lambda\\bigg(\\sum_{i}n_{u_{i}}\\big\\|u_{i}\\big\\|^{2}+\\sum_{j}n_{m_{j}}\\big\\|m_{j}\\big\\|^{2}\\bigg)$$\n",
    "\n",
    "where \n",
    "\n",
    "$n_{u_{i}}$ is the number of ratings of user $i$ \n",
    "\n",
    "$n_{m_{j}}$ denotes the number of ratings of movie $j$ \n",
    "\n",
    "$I_{j}$ denotes the set of users who rated movie $j$\n",
    "\n",
    "$I_{i}$ is the set of movies $j$ that user $i$ rated\n",
    "\n",
    "$n_{u_{i}}$ is the cardinality of $I_{i}$\n",
    "\n",
    "$n_{m_{j}}$ is the cardinality of $I_{j}$ \n",
    "\n",
    "This corresponds to Tikhonov regularization where $\\Gamma_{U}=\\text{diag}(n_{u_{i}})$ and $\\Gamma_{M}=\\text{diag}(n_{m_{j}})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "In this section, we use PySpark to create a distributed version of our SVD-based recommender system from Project 3. We do not create a full cluster, but instead run a local single node version, focusing on the implementation in PySpark rather than on cluster setup.\n",
    "\n",
    "Although the objective function of the model is the same as that used in Project 3 for probablistic matrix factorization, the Spark version of the model uses alternating least squares (ALS) to do the matrix factorization (i.e., solve for the user and item latent factors) rather than gradient descent. This method allows relatively easy scaling in a distributed environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Matrix Factorization Using Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Frist we review the results of the probabilistic matrix factorization using stochastic gradient descent from project 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from Surprise package\n",
    "from surprise import Reader, Dataset, SVD, accuracy\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import train_test_split\n",
    "# import python packages\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import random\n",
    "from math import sqrt\n",
    "\n",
    "# set random seed\n",
    "randomSeed = 12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the data object required for the modeling below as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reader library\n",
    "reader = Reader()\n",
    "# load ratings dataset with Dataset library\n",
    "data = Dataset.load_from_df(df[['user_id', 'item_id',\n",
    "  'rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed to ensure that our results are comparable across models and parameters and perform 5-fold cross validation using default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9372  0.9367  0.9382  0.9338  0.9358  0.9363  0.0015  \n",
      "MAE (testset)     0.7393  0.7408  0.7374  0.7367  0.7383  0.7385  0.0015  \n",
      "RMSE (trainset)   0.6857  0.6826  0.6863  0.6841  0.6846  0.6847  0.0013  \n",
      "MAE (trainset)    0.5435  0.5407  0.5447  0.5425  0.5424  0.5428  0.0013  \n",
      "Fit time          5.57    5.58    5.53    5.52    5.51    5.54    0.03    \n",
      "Test time         0.23    0.17    0.23    0.18    0.17    0.19    0.03    \n"
     ]
    }
   ],
   "source": [
    "random.seed(randomSeed)\n",
    "np.random.seed(randomSeed)\n",
    "\n",
    "# select SVD algorithm\n",
    "algo = SVD()\n",
    "\n",
    "# compute RMSE and MAE of SVD algorithm using 5-fold cross \n",
    "# validation\n",
    "result=cross_validate(algo, data, measures=['RMSE', 'MAE'], \n",
    "    cv=5, verbose=True,return_train_measures=True)\n",
    "# extract mean test RMSE\n",
    "meanTestRMSE=result[ 'test_rmse'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function to return the top-N recommendations for each user from a set of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    # return top-N recommendations for each user from \n",
    "    # a set of predictions.\n",
    "    #\n",
    "    # predictions(list of Prediction objects): list of \n",
    "    #   predictions, as returned by test method of an \n",
    "    #   algorithm.\n",
    "    # n(int): number of recommendation to output for each \n",
    "    #   user\n",
    "\n",
    "    # First map predictions to each user\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # sort predictions for each user and retrieve \n",
    "    # k highest ones\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], \n",
    "          reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    # return dict where keys are user (raw) ids and \n",
    "    # values are lists of tuples:\n",
    "    # [(raw item id, rating estimation), ...] of size n.\n",
    "    return top_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search...\n"
     ]
    }
   ],
   "source": [
    "random.seed(randomSeed)\n",
    "np.random.seed(randomSeed)\n",
    "# select best algo with grid search\n",
    "print('Grid Search...')\n",
    "param_grid = {'n_epochs': [10, 20, 30], \n",
    "    'lr_all': [0.002, 0.005, 0.01,0.015,0.02,0.05],\n",
    "    'biased' : [True,False]}\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], \n",
    "  cv=5)\n",
    "\n",
    "random.seed(randomSeed)\n",
    "np.random.seed(randomSeed)\n",
    "# fit using best model\n",
    "grid_search.fit(data)\n",
    "# use best from grid search\n",
    "algo = grid_search.best_estimator['rmse']\n",
    "bestRMSE = grid_search.best_score['rmse']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9363329339678333"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9363396424407643"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanTestRMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the grid search performance, our best mean RMSE is 0.9363 across all 5-folds matching the performance of the default settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "results_df = pd.DataFrame.from_dict(grid_search.cv_results)\n",
    "# extract required columns\n",
    "results_table=results_df[['param_biased','param_lr_all',\n",
    "    'param_n_epochs','mean_test_rmse','split0_test_rmse',\n",
    "    'split1_test_rmse','split2_test_rmse','split3_test_rmse',\n",
    "    'split4_test_rmse']].sort_values(by=['mean_test_rmse'])\n",
    "columnMap={'param_biased' : 'biased',\n",
    "    'param_lr_all' : 'lr_all',\n",
    "    'param_n_epochs' : 'n_epochs',\n",
    "    'mean_test_rmse' : 'Mean',\n",
    "    'split0_test_rmse' : 'Fold 1', \n",
    "    'split1_test_rmse' : 'Fold 2',\n",
    "    'split2_test_rmse' : 'Fold 3', \n",
    "    'split3_test_rmse' : 'Fold 4',\n",
    "    'split4_test_rmse' : 'Fold 5'}\n",
    "results_table.rename(columns=columnMap,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biased</th>\n",
       "      <th>lr_all</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Fold 4</th>\n",
       "      <th>Fold 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>0.010</td>\n",
       "      <td>10</td>\n",
       "      <td>0.936333</td>\n",
       "      <td>0.936610</td>\n",
       "      <td>0.938120</td>\n",
       "      <td>0.939056</td>\n",
       "      <td>0.934613</td>\n",
       "      <td>0.933266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>True</td>\n",
       "      <td>0.005</td>\n",
       "      <td>20</td>\n",
       "      <td>0.936745</td>\n",
       "      <td>0.935265</td>\n",
       "      <td>0.936795</td>\n",
       "      <td>0.942951</td>\n",
       "      <td>0.936236</td>\n",
       "      <td>0.932477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>0.015</td>\n",
       "      <td>10</td>\n",
       "      <td>0.943420</td>\n",
       "      <td>0.945438</td>\n",
       "      <td>0.939999</td>\n",
       "      <td>0.948176</td>\n",
       "      <td>0.941976</td>\n",
       "      <td>0.941512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>True</td>\n",
       "      <td>0.005</td>\n",
       "      <td>30</td>\n",
       "      <td>0.943580</td>\n",
       "      <td>0.947548</td>\n",
       "      <td>0.940927</td>\n",
       "      <td>0.945648</td>\n",
       "      <td>0.946101</td>\n",
       "      <td>0.937675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>True</td>\n",
       "      <td>0.002</td>\n",
       "      <td>30</td>\n",
       "      <td>0.944294</td>\n",
       "      <td>0.944029</td>\n",
       "      <td>0.944479</td>\n",
       "      <td>0.949286</td>\n",
       "      <td>0.942290</td>\n",
       "      <td>0.941385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.947223</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.946636</td>\n",
       "      <td>0.951514</td>\n",
       "      <td>0.945079</td>\n",
       "      <td>0.945252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>0.005</td>\n",
       "      <td>20</td>\n",
       "      <td>0.948613</td>\n",
       "      <td>0.951853</td>\n",
       "      <td>0.945324</td>\n",
       "      <td>0.954849</td>\n",
       "      <td>0.943987</td>\n",
       "      <td>0.947055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>0.002</td>\n",
       "      <td>20</td>\n",
       "      <td>0.949987</td>\n",
       "      <td>0.950142</td>\n",
       "      <td>0.948434</td>\n",
       "      <td>0.956086</td>\n",
       "      <td>0.948348</td>\n",
       "      <td>0.946926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>0.010</td>\n",
       "      <td>10</td>\n",
       "      <td>0.953025</td>\n",
       "      <td>0.953494</td>\n",
       "      <td>0.948194</td>\n",
       "      <td>0.963160</td>\n",
       "      <td>0.948255</td>\n",
       "      <td>0.952023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>False</td>\n",
       "      <td>0.005</td>\n",
       "      <td>30</td>\n",
       "      <td>0.953290</td>\n",
       "      <td>0.955076</td>\n",
       "      <td>0.947399</td>\n",
       "      <td>0.962642</td>\n",
       "      <td>0.950337</td>\n",
       "      <td>0.950995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>0.020</td>\n",
       "      <td>10</td>\n",
       "      <td>0.953994</td>\n",
       "      <td>0.954803</td>\n",
       "      <td>0.951045</td>\n",
       "      <td>0.959488</td>\n",
       "      <td>0.953477</td>\n",
       "      <td>0.951158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>True</td>\n",
       "      <td>0.010</td>\n",
       "      <td>20</td>\n",
       "      <td>0.954422</td>\n",
       "      <td>0.958067</td>\n",
       "      <td>0.956654</td>\n",
       "      <td>0.956745</td>\n",
       "      <td>0.953249</td>\n",
       "      <td>0.947398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>0.015</td>\n",
       "      <td>10</td>\n",
       "      <td>0.959367</td>\n",
       "      <td>0.960720</td>\n",
       "      <td>0.954012</td>\n",
       "      <td>0.969482</td>\n",
       "      <td>0.954134</td>\n",
       "      <td>0.958486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>0.002</td>\n",
       "      <td>10</td>\n",
       "      <td>0.962926</td>\n",
       "      <td>0.962685</td>\n",
       "      <td>0.962161</td>\n",
       "      <td>0.968472</td>\n",
       "      <td>0.960847</td>\n",
       "      <td>0.960465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>False</td>\n",
       "      <td>0.002</td>\n",
       "      <td>30</td>\n",
       "      <td>0.967097</td>\n",
       "      <td>0.970785</td>\n",
       "      <td>0.960351</td>\n",
       "      <td>0.975844</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.966006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>False</td>\n",
       "      <td>0.010</td>\n",
       "      <td>20</td>\n",
       "      <td>0.969302</td>\n",
       "      <td>0.976076</td>\n",
       "      <td>0.961909</td>\n",
       "      <td>0.975329</td>\n",
       "      <td>0.967677</td>\n",
       "      <td>0.965518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>True</td>\n",
       "      <td>0.015</td>\n",
       "      <td>20</td>\n",
       "      <td>0.972670</td>\n",
       "      <td>0.976187</td>\n",
       "      <td>0.971035</td>\n",
       "      <td>0.976668</td>\n",
       "      <td>0.968696</td>\n",
       "      <td>0.970766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>True</td>\n",
       "      <td>0.010</td>\n",
       "      <td>30</td>\n",
       "      <td>0.973203</td>\n",
       "      <td>0.970939</td>\n",
       "      <td>0.968667</td>\n",
       "      <td>0.982860</td>\n",
       "      <td>0.972735</td>\n",
       "      <td>0.970814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>0.020</td>\n",
       "      <td>10</td>\n",
       "      <td>0.975298</td>\n",
       "      <td>0.977871</td>\n",
       "      <td>0.970301</td>\n",
       "      <td>0.976564</td>\n",
       "      <td>0.975495</td>\n",
       "      <td>0.976260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>True</td>\n",
       "      <td>0.020</td>\n",
       "      <td>20</td>\n",
       "      <td>0.978208</td>\n",
       "      <td>0.983180</td>\n",
       "      <td>0.975081</td>\n",
       "      <td>0.980732</td>\n",
       "      <td>0.978390</td>\n",
       "      <td>0.973658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.979137</td>\n",
       "      <td>0.982012</td>\n",
       "      <td>0.970966</td>\n",
       "      <td>0.988101</td>\n",
       "      <td>0.976070</td>\n",
       "      <td>0.978536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>True</td>\n",
       "      <td>0.020</td>\n",
       "      <td>30</td>\n",
       "      <td>0.980968</td>\n",
       "      <td>0.981850</td>\n",
       "      <td>0.977825</td>\n",
       "      <td>0.986564</td>\n",
       "      <td>0.980440</td>\n",
       "      <td>0.978162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>True</td>\n",
       "      <td>0.015</td>\n",
       "      <td>30</td>\n",
       "      <td>0.981131</td>\n",
       "      <td>0.984925</td>\n",
       "      <td>0.975812</td>\n",
       "      <td>0.985740</td>\n",
       "      <td>0.976641</td>\n",
       "      <td>0.982540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>True</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30</td>\n",
       "      <td>0.982437</td>\n",
       "      <td>0.987275</td>\n",
       "      <td>0.976406</td>\n",
       "      <td>0.989304</td>\n",
       "      <td>0.980263</td>\n",
       "      <td>0.978937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>False</td>\n",
       "      <td>0.010</td>\n",
       "      <td>30</td>\n",
       "      <td>0.983669</td>\n",
       "      <td>0.988109</td>\n",
       "      <td>0.977487</td>\n",
       "      <td>0.991606</td>\n",
       "      <td>0.978516</td>\n",
       "      <td>0.982626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>True</td>\n",
       "      <td>0.050</td>\n",
       "      <td>20</td>\n",
       "      <td>0.985626</td>\n",
       "      <td>0.988108</td>\n",
       "      <td>0.980631</td>\n",
       "      <td>0.993561</td>\n",
       "      <td>0.983065</td>\n",
       "      <td>0.982766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>False</td>\n",
       "      <td>0.015</td>\n",
       "      <td>20</td>\n",
       "      <td>0.986083</td>\n",
       "      <td>0.989636</td>\n",
       "      <td>0.978324</td>\n",
       "      <td>0.998357</td>\n",
       "      <td>0.982093</td>\n",
       "      <td>0.982003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>True</td>\n",
       "      <td>0.050</td>\n",
       "      <td>10</td>\n",
       "      <td>0.988999</td>\n",
       "      <td>0.997351</td>\n",
       "      <td>0.984555</td>\n",
       "      <td>0.996194</td>\n",
       "      <td>0.979529</td>\n",
       "      <td>0.987369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>False</td>\n",
       "      <td>0.015</td>\n",
       "      <td>30</td>\n",
       "      <td>0.992236</td>\n",
       "      <td>0.994206</td>\n",
       "      <td>0.988092</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.987440</td>\n",
       "      <td>0.992275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>False</td>\n",
       "      <td>0.020</td>\n",
       "      <td>20</td>\n",
       "      <td>0.994096</td>\n",
       "      <td>0.992611</td>\n",
       "      <td>0.987731</td>\n",
       "      <td>1.003313</td>\n",
       "      <td>0.991721</td>\n",
       "      <td>0.995104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>False</td>\n",
       "      <td>0.020</td>\n",
       "      <td>30</td>\n",
       "      <td>0.994384</td>\n",
       "      <td>0.995982</td>\n",
       "      <td>0.987855</td>\n",
       "      <td>1.003235</td>\n",
       "      <td>0.990891</td>\n",
       "      <td>0.993960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>0.002</td>\n",
       "      <td>20</td>\n",
       "      <td>1.001625</td>\n",
       "      <td>1.005750</td>\n",
       "      <td>0.991381</td>\n",
       "      <td>1.008731</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>1.002583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>False</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30</td>\n",
       "      <td>1.004988</td>\n",
       "      <td>1.012488</td>\n",
       "      <td>0.998750</td>\n",
       "      <td>1.012452</td>\n",
       "      <td>0.998988</td>\n",
       "      <td>1.002261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>False</td>\n",
       "      <td>0.050</td>\n",
       "      <td>20</td>\n",
       "      <td>1.010499</td>\n",
       "      <td>1.011042</td>\n",
       "      <td>1.005629</td>\n",
       "      <td>1.017178</td>\n",
       "      <td>1.008806</td>\n",
       "      <td>1.009838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>0.050</td>\n",
       "      <td>10</td>\n",
       "      <td>1.022581</td>\n",
       "      <td>1.025387</td>\n",
       "      <td>1.012343</td>\n",
       "      <td>1.033724</td>\n",
       "      <td>1.015264</td>\n",
       "      <td>1.026190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.002</td>\n",
       "      <td>10</td>\n",
       "      <td>1.171242</td>\n",
       "      <td>1.171412</td>\n",
       "      <td>1.154134</td>\n",
       "      <td>1.176080</td>\n",
       "      <td>1.175850</td>\n",
       "      <td>1.178736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    biased  lr_all  n_epochs      Mean    Fold 1    Fold 2    Fold 3  \\\n",
       "4     True   0.010        10  0.936333  0.936610  0.938120  0.939056   \n",
       "14    True   0.005        20  0.936745  0.935265  0.936795  0.942951   \n",
       "6     True   0.015        10  0.943420  0.945438  0.939999  0.948176   \n",
       "26    True   0.005        30  0.943580  0.947548  0.940927  0.945648   \n",
       "24    True   0.002        30  0.944294  0.944029  0.944479  0.949286   \n",
       "2     True   0.005        10  0.947223  0.947633  0.946636  0.951514   \n",
       "15   False   0.005        20  0.948613  0.951853  0.945324  0.954849   \n",
       "12    True   0.002        20  0.949987  0.950142  0.948434  0.956086   \n",
       "5    False   0.010        10  0.953025  0.953494  0.948194  0.963160   \n",
       "27   False   0.005        30  0.953290  0.955076  0.947399  0.962642   \n",
       "8     True   0.020        10  0.953994  0.954803  0.951045  0.959488   \n",
       "16    True   0.010        20  0.954422  0.958067  0.956654  0.956745   \n",
       "7    False   0.015        10  0.959367  0.960720  0.954012  0.969482   \n",
       "0     True   0.002        10  0.962926  0.962685  0.962161  0.968472   \n",
       "25   False   0.002        30  0.967097  0.970785  0.960351  0.975844   \n",
       "17   False   0.010        20  0.969302  0.976076  0.961909  0.975329   \n",
       "18    True   0.015        20  0.972670  0.976187  0.971035  0.976668   \n",
       "28    True   0.010        30  0.973203  0.970939  0.968667  0.982860   \n",
       "9    False   0.020        10  0.975298  0.977871  0.970301  0.976564   \n",
       "20    True   0.020        20  0.978208  0.983180  0.975081  0.980732   \n",
       "3    False   0.005        10  0.979137  0.982012  0.970966  0.988101   \n",
       "32    True   0.020        30  0.980968  0.981850  0.977825  0.986564   \n",
       "30    True   0.015        30  0.981131  0.984925  0.975812  0.985740   \n",
       "34    True   0.050        30  0.982437  0.987275  0.976406  0.989304   \n",
       "29   False   0.010        30  0.983669  0.988109  0.977487  0.991606   \n",
       "22    True   0.050        20  0.985626  0.988108  0.980631  0.993561   \n",
       "19   False   0.015        20  0.986083  0.989636  0.978324  0.998357   \n",
       "10    True   0.050        10  0.988999  0.997351  0.984555  0.996194   \n",
       "31   False   0.015        30  0.992236  0.994206  0.988092  0.999166   \n",
       "21   False   0.020        20  0.994096  0.992611  0.987731  1.003313   \n",
       "33   False   0.020        30  0.994384  0.995982  0.987855  1.003235   \n",
       "13   False   0.002        20  1.001625  1.005750  0.991381  1.008731   \n",
       "35   False   0.050        30  1.004988  1.012488  0.998750  1.012452   \n",
       "23   False   0.050        20  1.010499  1.011042  1.005629  1.017178   \n",
       "11   False   0.050        10  1.022581  1.025387  1.012343  1.033724   \n",
       "1    False   0.002        10  1.171242  1.171412  1.154134  1.176080   \n",
       "\n",
       "      Fold 4    Fold 5  \n",
       "4   0.934613  0.933266  \n",
       "14  0.936236  0.932477  \n",
       "6   0.941976  0.941512  \n",
       "26  0.946101  0.937675  \n",
       "24  0.942290  0.941385  \n",
       "2   0.945079  0.945252  \n",
       "15  0.943987  0.947055  \n",
       "12  0.948348  0.946926  \n",
       "5   0.948255  0.952023  \n",
       "27  0.950337  0.950995  \n",
       "8   0.953477  0.951158  \n",
       "16  0.953249  0.947398  \n",
       "7   0.954134  0.958486  \n",
       "0   0.960847  0.960465  \n",
       "25  0.962500  0.966006  \n",
       "17  0.967677  0.965518  \n",
       "18  0.968696  0.970766  \n",
       "28  0.972735  0.970814  \n",
       "9   0.975495  0.976260  \n",
       "20  0.978390  0.973658  \n",
       "3   0.976070  0.978536  \n",
       "32  0.980440  0.978162  \n",
       "30  0.976641  0.982540  \n",
       "34  0.980263  0.978937  \n",
       "29  0.978516  0.982626  \n",
       "22  0.983065  0.982766  \n",
       "19  0.982093  0.982003  \n",
       "10  0.979529  0.987369  \n",
       "31  0.987440  0.992275  \n",
       "21  0.991721  0.995104  \n",
       "33  0.990891  0.993960  \n",
       "13  0.999679  1.002583  \n",
       "35  0.998988  1.002261  \n",
       "23  1.008806  1.009838  \n",
       "11  1.015264  1.026190  \n",
       "1   1.175850  1.178736  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 'biased' column indicates whether we include biases $b_i$ an and $b_u$ in our model (i.e., whether or not we use probabilistic matrix factorization or the extended probabilistic matrix factorization).\n",
    "\n",
    "- 'lr_all' is parameter is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we exclude the extended probabilistic matrix factorization model results, the performance worsens to 0.9486:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biased</th>\n",
       "      <th>lr_all</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Fold 4</th>\n",
       "      <th>Fold 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>False</td>\n",
       "      <td>0.005</td>\n",
       "      <td>20</td>\n",
       "      <td>0.948613</td>\n",
       "      <td>0.951853</td>\n",
       "      <td>0.945324</td>\n",
       "      <td>0.954849</td>\n",
       "      <td>0.943987</td>\n",
       "      <td>0.947055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>0.010</td>\n",
       "      <td>10</td>\n",
       "      <td>0.953025</td>\n",
       "      <td>0.953494</td>\n",
       "      <td>0.948194</td>\n",
       "      <td>0.963160</td>\n",
       "      <td>0.948255</td>\n",
       "      <td>0.952023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>False</td>\n",
       "      <td>0.005</td>\n",
       "      <td>30</td>\n",
       "      <td>0.953290</td>\n",
       "      <td>0.955076</td>\n",
       "      <td>0.947399</td>\n",
       "      <td>0.962642</td>\n",
       "      <td>0.950337</td>\n",
       "      <td>0.950995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>0.015</td>\n",
       "      <td>10</td>\n",
       "      <td>0.959367</td>\n",
       "      <td>0.960720</td>\n",
       "      <td>0.954012</td>\n",
       "      <td>0.969482</td>\n",
       "      <td>0.954134</td>\n",
       "      <td>0.958486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>False</td>\n",
       "      <td>0.002</td>\n",
       "      <td>30</td>\n",
       "      <td>0.967097</td>\n",
       "      <td>0.970785</td>\n",
       "      <td>0.960351</td>\n",
       "      <td>0.975844</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.966006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>False</td>\n",
       "      <td>0.010</td>\n",
       "      <td>20</td>\n",
       "      <td>0.969302</td>\n",
       "      <td>0.976076</td>\n",
       "      <td>0.961909</td>\n",
       "      <td>0.975329</td>\n",
       "      <td>0.967677</td>\n",
       "      <td>0.965518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>0.020</td>\n",
       "      <td>10</td>\n",
       "      <td>0.975298</td>\n",
       "      <td>0.977871</td>\n",
       "      <td>0.970301</td>\n",
       "      <td>0.976564</td>\n",
       "      <td>0.975495</td>\n",
       "      <td>0.976260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.005</td>\n",
       "      <td>10</td>\n",
       "      <td>0.979137</td>\n",
       "      <td>0.982012</td>\n",
       "      <td>0.970966</td>\n",
       "      <td>0.988101</td>\n",
       "      <td>0.976070</td>\n",
       "      <td>0.978536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>False</td>\n",
       "      <td>0.010</td>\n",
       "      <td>30</td>\n",
       "      <td>0.983669</td>\n",
       "      <td>0.988109</td>\n",
       "      <td>0.977487</td>\n",
       "      <td>0.991606</td>\n",
       "      <td>0.978516</td>\n",
       "      <td>0.982626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>False</td>\n",
       "      <td>0.015</td>\n",
       "      <td>20</td>\n",
       "      <td>0.986083</td>\n",
       "      <td>0.989636</td>\n",
       "      <td>0.978324</td>\n",
       "      <td>0.998357</td>\n",
       "      <td>0.982093</td>\n",
       "      <td>0.982003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>False</td>\n",
       "      <td>0.015</td>\n",
       "      <td>30</td>\n",
       "      <td>0.992236</td>\n",
       "      <td>0.994206</td>\n",
       "      <td>0.988092</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>0.987440</td>\n",
       "      <td>0.992275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>False</td>\n",
       "      <td>0.020</td>\n",
       "      <td>20</td>\n",
       "      <td>0.994096</td>\n",
       "      <td>0.992611</td>\n",
       "      <td>0.987731</td>\n",
       "      <td>1.003313</td>\n",
       "      <td>0.991721</td>\n",
       "      <td>0.995104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>False</td>\n",
       "      <td>0.020</td>\n",
       "      <td>30</td>\n",
       "      <td>0.994384</td>\n",
       "      <td>0.995982</td>\n",
       "      <td>0.987855</td>\n",
       "      <td>1.003235</td>\n",
       "      <td>0.990891</td>\n",
       "      <td>0.993960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>0.002</td>\n",
       "      <td>20</td>\n",
       "      <td>1.001625</td>\n",
       "      <td>1.005750</td>\n",
       "      <td>0.991381</td>\n",
       "      <td>1.008731</td>\n",
       "      <td>0.999679</td>\n",
       "      <td>1.002583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>False</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30</td>\n",
       "      <td>1.004988</td>\n",
       "      <td>1.012488</td>\n",
       "      <td>0.998750</td>\n",
       "      <td>1.012452</td>\n",
       "      <td>0.998988</td>\n",
       "      <td>1.002261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>False</td>\n",
       "      <td>0.050</td>\n",
       "      <td>20</td>\n",
       "      <td>1.010499</td>\n",
       "      <td>1.011042</td>\n",
       "      <td>1.005629</td>\n",
       "      <td>1.017178</td>\n",
       "      <td>1.008806</td>\n",
       "      <td>1.009838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>False</td>\n",
       "      <td>0.050</td>\n",
       "      <td>10</td>\n",
       "      <td>1.022581</td>\n",
       "      <td>1.025387</td>\n",
       "      <td>1.012343</td>\n",
       "      <td>1.033724</td>\n",
       "      <td>1.015264</td>\n",
       "      <td>1.026190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0.002</td>\n",
       "      <td>10</td>\n",
       "      <td>1.171242</td>\n",
       "      <td>1.171412</td>\n",
       "      <td>1.154134</td>\n",
       "      <td>1.176080</td>\n",
       "      <td>1.175850</td>\n",
       "      <td>1.178736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    biased  lr_all  n_epochs      Mean    Fold 1    Fold 2    Fold 3  \\\n",
       "15   False   0.005        20  0.948613  0.951853  0.945324  0.954849   \n",
       "5    False   0.010        10  0.953025  0.953494  0.948194  0.963160   \n",
       "27   False   0.005        30  0.953290  0.955076  0.947399  0.962642   \n",
       "7    False   0.015        10  0.959367  0.960720  0.954012  0.969482   \n",
       "25   False   0.002        30  0.967097  0.970785  0.960351  0.975844   \n",
       "17   False   0.010        20  0.969302  0.976076  0.961909  0.975329   \n",
       "9    False   0.020        10  0.975298  0.977871  0.970301  0.976564   \n",
       "3    False   0.005        10  0.979137  0.982012  0.970966  0.988101   \n",
       "29   False   0.010        30  0.983669  0.988109  0.977487  0.991606   \n",
       "19   False   0.015        20  0.986083  0.989636  0.978324  0.998357   \n",
       "31   False   0.015        30  0.992236  0.994206  0.988092  0.999166   \n",
       "21   False   0.020        20  0.994096  0.992611  0.987731  1.003313   \n",
       "33   False   0.020        30  0.994384  0.995982  0.987855  1.003235   \n",
       "13   False   0.002        20  1.001625  1.005750  0.991381  1.008731   \n",
       "35   False   0.050        30  1.004988  1.012488  0.998750  1.012452   \n",
       "23   False   0.050        20  1.010499  1.011042  1.005629  1.017178   \n",
       "11   False   0.050        10  1.022581  1.025387  1.012343  1.033724   \n",
       "1    False   0.002        10  1.171242  1.171412  1.154134  1.176080   \n",
       "\n",
       "      Fold 4    Fold 5  \n",
       "15  0.943987  0.947055  \n",
       "5   0.948255  0.952023  \n",
       "27  0.950337  0.950995  \n",
       "7   0.954134  0.958486  \n",
       "25  0.962500  0.966006  \n",
       "17  0.967677  0.965518  \n",
       "9   0.975495  0.976260  \n",
       "3   0.976070  0.978536  \n",
       "29  0.978516  0.982626  \n",
       "19  0.982093  0.982003  \n",
       "31  0.987440  0.992275  \n",
       "21  0.991721  0.995104  \n",
       "33  0.990891  0.993960  \n",
       "13  0.999679  1.002583  \n",
       "35  0.998988  1.002261  \n",
       "23  1.008806  1.009838  \n",
       "11  1.015264  1.026190  \n",
       "1   1.175850  1.178736  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonBiasedIndex=results_table['biased']==False\n",
    "results_table.loc[nonBiasedIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have looked at the 5-fold cross validated performance for a range of parameters, we demonstrate how the model can be used to produce recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training / testing split\n",
    "testTrainSplit=0.75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train an extended probabilistic matrix factorization model in-sample using 75% of the data, then we generate recommendations for all of the out-of-sample users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9758846020775652"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(randomSeed)\n",
    "np.random.seed(randomSeed)\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 75% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=testTrainSplit)\n",
    "# create predictions from\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define top N \n",
    "topN=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find the top 10 recommendations for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "iid='item_id'\n",
    "# map item id to title \n",
    "item2Title=df_title_by_item.set_index(iid)['title'].to_dict()\n",
    "# get top 10 recommendation for each user\n",
    "top_n = get_top_n(predictions, n=topN)\n",
    "# extract top N recommendations for each user\n",
    "listTopN=list()\n",
    "for uid, user_ratings in top_n.items():\n",
    "    listTopN.append([uid,[item2Title[int(iid)] for (iid, \n",
    "    _) in user_ratings]])\n",
    "# convert to dataframe\n",
    "dfTopN=pd.DataFrame(listTopN)\n",
    "# rename columns\n",
    "dfTopN.rename(columns={0 : 'user_id',1 : 'recommendations'},\n",
    "    inplace=True)\n",
    "# set index to user ID\n",
    "dfTopN=dfTopN.set_index('user_id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the recommendations out-of-sample. We can predict the rating for an item by a user that was not part of the training set and compare that prediction with their actual rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 531        item: 421        r_ui = 4.00   est = 3.53   {'was_impossible': False}\n",
      "user: 531        item: 421        r_ui = 4.00   est = 3.53   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "# define raw user id (string)\n",
    "user_id = str(531)\n",
    "# define raw item id (string)\n",
    "item_id = str(421)  \n",
    "trueRating=4.0\n",
    "# get a prediction for specific users and items.\n",
    "pred = algo.predict(user_id, item_id, r_ui=trueRating, \n",
    "  verbose=True)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           recommendations\n",
      "user_id                                                   \n",
      "521      [Usual Suspects, The (1995), Star Wars (1977),...\n",
      "405      [Shawshank Redemption, The (1994), Rear Window...\n",
      "216      [Star Wars (1977), Usual Suspects, The (1995),...\n",
      "478      [Star Wars (1977), Shawshank Redemption, The (...\n",
      "454      [Rear Window (1954), One Flew Over the Cuckoo'...\n"
     ]
    }
   ],
   "source": [
    "print(dfTopN.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the top 10 recommendations for an example user as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define example user\n",
    "user_id=5\n",
    "# extract top N recommendations for example user\n",
    "recommendationList=dfTopN['recommendations'].loc[user_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, here are the recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             recommendations\n",
      "0                      Close Shave, A (1995)\n",
      "1                           Star Wars (1977)\n",
      "2             Raiders of the Lost Ark (1981)\n",
      "3                 Princess Bride, The (1987)\n",
      "4           Silence of the Lambs, The (1991)\n",
      "5               To Kill a Mockingbird (1962)\n",
      "6                                Jaws (1975)\n",
      "7     Monty Python and the Holy Grail (1974)\n",
      "8                  Young Frankenstein (1974)\n",
      "9  Indiana Jones and the Last Crusade (1989)\n"
     ]
    }
   ],
   "source": [
    "sampleRecommendations=pd.DataFrame(recommendationList,\n",
    "  columns=['recommendations'])\n",
    "print(sampleRecommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have not delved into measures beyond the root mean squared error to understand the utility of these recommendations to our users, there are many more techniques that can be used to help us understand our data once we have the latent factor loadings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Matrix Factorization Using Alternating Least Squares (ALS) With Weighted-$\\lambda$-Regularization\n",
    "\n",
    "Next, we review the results of the probabilistic matrix factorization using alternating least squares (ALS) with weighted-$\\lambda$-regularization. We use the PySpark implementation which can be scaled out using a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create shape method for spark dataframe\n",
    "def spark_shape(self):\n",
    "    return (self.count(), len(self.columns))\n",
    "pyspark.sql.dataframe.DataFrame.shape = spark_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed and apply a random split, using 75% of the data for training and 25% of the data for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split (75%/25%)\n",
    "X_train, X_test = s_df.randomSplit([0.75, 0.25],seed=randomSeed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We double-check the dimension of the training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79972, 5)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20028, 5)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_id=1, title='Toy Story (1995)', user_id=1, rating=5, titmestamp=874965758),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=5, rating=4, titmestamp=875635748),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=6, rating=4, titmestamp=883599478),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=13, rating=3, titmestamp=882140487),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=15, rating=1, titmestamp=879455635)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_id=1, title='Toy Story (1995)', user_id=2, rating=4, titmestamp=888550871),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=10, rating=4, titmestamp=877888877),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=16, rating=5, titmestamp=877717833),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=18, rating=5, titmestamp=880130802),\n",
       " Row(item_id=1, title='Toy Story (1995)', user_id=20, rating=3, titmestamp=879667963)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the select method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=1, item_id=1, rating=5),\n",
       " Row(user_id=5, item_id=1, rating=4),\n",
       " Row(user_id=6, item_id=1, rating=4),\n",
       " Row(user_id=13, item_id=1, rating=3),\n",
       " Row(user_id=15, item_id=1, rating=1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.select([\"user_id\", \"item_id\", \"rating\"]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test simple model fitting on the training data using sample hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "# define ALS inital parameters\n",
    "maxIterations=5\n",
    "# define number of latent features\n",
    "k=10\n",
    "regParameter=0.01\n",
    "# define model\n",
    "als = ALS(maxIter=maxIterations, rank=k, regParam=regParameter, \n",
    "    userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",nonnegative=True,implicitPrefs=False,seed=randomSeed)\n",
    "# fit model\n",
    "model = als.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluation our model out-of-sample using the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error (RMSE) = 0.9995291737343103\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# evaluate model (compute RMSE on test data)\n",
    "predictions = model.transform(X_test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error (RMSE) = \" + str(rmse))\n",
    "\n",
    "# generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# generate top 10 user recommendations for each item\n",
    "itemRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any parameter tuning, we achieve a RMSE of roughly 1.\n",
    "\n",
    "We generate the top 10 movie recommendations for a specified set of users and for a specified set of movies as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate top 10 movie recommendations for a specified set of users\n",
    "users = s_df.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "# generate top 10 user recommendations for a specified set of movies\n",
    "items =s_df.select(als.getItemCol()).distinct().limit(3)\n",
    "itemSubsetRecs = model.recommendForItemSubset(items, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=26, recommendations=[Row(item_id=1368, rating=5.025524616241455), Row(item_id=1463, rating=4.919097900390625), Row(item_id=1449, rating=4.811394214630127), Row(item_id=1397, rating=4.704085350036621), Row(item_id=1427, rating=4.645888328552246), Row(item_id=1452, rating=4.438278675079346), Row(item_id=1458, rating=4.438278675079346), Row(item_id=1155, rating=4.377913951873779), Row(item_id=534, rating=4.334097862243652), Row(item_id=113, rating=4.311575889587402)]),\n",
       " Row(user_id=474, recommendations=[Row(item_id=1463, rating=8.219408988952637), Row(item_id=1394, rating=5.69248104095459), Row(item_id=1449, rating=5.663028240203857), Row(item_id=626, rating=5.428760051727295), Row(item_id=1558, rating=5.241504669189453), Row(item_id=1639, rating=5.227160930633545), Row(item_id=1645, rating=5.179046154022217), Row(item_id=1650, rating=5.179046154022217), Row(item_id=1585, rating=5.179046154022217), Row(item_id=1405, rating=5.120999813079834)]),\n",
       " Row(user_id=29, recommendations=[Row(item_id=1463, rating=7.988710403442383), Row(item_id=1394, rating=7.71291446685791), Row(item_id=1380, rating=7.538386344909668), Row(item_id=1128, rating=6.9341721534729), Row(item_id=1368, rating=6.853912353515625), Row(item_id=909, rating=6.824227809906006), Row(item_id=1558, rating=6.776157379150391), Row(item_id=296, rating=6.675416469573975), Row(item_id=915, rating=6.340188026428223), Row(item_id=1462, rating=6.186722755432129)])]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userSubsetRecs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(item_id=26, recommendations=[Row(user_id=863, rating=6.21252965927124), Row(user_id=609, rating=6.171199798583984), Row(user_id=260, rating=6.1130757331848145), Row(user_id=68, rating=6.040411472320557), Row(user_id=697, rating=5.93201208114624), Row(user_id=88, rating=5.791384696960449), Row(user_id=507, rating=5.740054607391357), Row(user_id=355, rating=5.581533432006836), Row(user_id=153, rating=5.5377960205078125), Row(user_id=775, rating=5.489548683166504)]),\n",
       " Row(item_id=474, recommendations=[Row(user_id=863, rating=7.1820502281188965), Row(user_id=310, rating=6.90852689743042), Row(user_id=803, rating=6.594876766204834), Row(user_id=273, rating=6.282232761383057), Row(user_id=353, rating=6.219252109527588), Row(user_id=68, rating=6.145155906677246), Row(user_id=720, rating=6.073712348937988), Row(user_id=895, rating=6.005275249481201), Row(user_id=697, rating=5.972476482391357), Row(user_id=818, rating=5.950571537017822)]),\n",
       " Row(item_id=29, recommendations=[Row(user_id=153, rating=8.82631778717041), Row(user_id=127, rating=7.527055740356445), Row(user_id=689, rating=7.325575828552246), Row(user_id=202, rating=7.295932769775391), Row(user_id=760, rating=7.108534812927246), Row(user_id=471, rating=6.9507293701171875), Row(user_id=438, rating=6.837264060974121), Row(user_id=260, rating=6.6986799240112305), Row(user_id=424, rating=6.648374557495117), Row(user_id=148, rating=6.645312309265137)])]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemSubsetRecs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have learned the basic methods for fitting and applying our model to generate recommendationss, we move to hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "In this section, we use the training data to evaluate a range of model hyperparameters. We iterate over a range of parameters and apply 5-fold cross-validation to select a model based on the lowest root mean squared error (RMSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build generic ALS model without hyperparameters\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\",coldStartStrategy=\"drop\", \n",
    "    nonnegative = True,implicitPrefs = False,seed=randomSeed)\n",
    "# Tell Spark what values to try for each hyperparameter\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().addGrid(als.rank, \n",
    "    [5, 40, 80, 120]).addGrid(als.maxIter, \n",
    "    [5, 10]).addGrid(als.regParam, \n",
    "    [.01,.05, .1, 1.5]).build()\n",
    "\n",
    "# Tell Spark how to evaluate model performance\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Build cross validation step using CrossValidator \n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "nFolds=5\n",
    "cv = CrossValidator(estimator = als, estimatorParamMaps = param_grid,evaluator = evaluator,\n",
    "    numFolds = nFolds,seed=randomSeed)\n",
    "\n",
    "# Run cross validation on training data\n",
    "model = cv.fit(X_train)\n",
    "# Extract best combination of values from cross validation\n",
    "best_model = model.bestModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on our local workstation we were not able to run a higher number of iterations without our training process crashing despite having 18 cores and 128 gigs of RAM. All of the cores are used during the parameter search, but RAM utilization remained low throughout.\n",
    "\n",
    "We generate the predicted ratings using the test set and compute the root mean squared error (RMSE) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test set predictions\n",
    "predictions = best_model.transform(X_test)\n",
    "# evaluate using RMSE\n",
    "rmseBest = evaluator.evaluate(predictions)\n",
    "kBest=best_model.rank\n",
    "maxNIterationsBest=best_model._java_obj.parent().getMaxIter()\n",
    "regParameterBest=best_model._java_obj.parent().getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE: 0.9097099852148802\n",
      "Best # of Latent Factors: 120\n",
      "Best Max # of Iterations: 10\n",
      "Best Reg Parameter: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Best RMSE: \"+str(rmseBest))\n",
    "print(\"Best # of Latent Factors: \"+str(kBest))\n",
    "print(\"Best Max # of Iterations: \"+str(maxNIterationsBest))\n",
    "print(\"Best Reg Parameter: \"+str(regParameterBest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model - which uses 120 factors, a maximum of 10 iterations, and a regularization parameter of 0.1 - has an root mean squared error of 0.9097 on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the latent factors - both user and item - as for our best model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract best model latent user factors\n",
    "userFactors=best_model.userFactors.orderBy('id')\n",
    "# extract best model latent item factors\n",
    "itemFactors=best_model.itemFactors.orderBy('id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  1|[0.009767955, 0.0...|\n",
      "|  2|[0.06330102, 0.0,...|\n",
      "|  3|[0.00480999, 0.0,...|\n",
      "|  4|[0.037397504, 0.1...|\n",
      "|  5|[0.0, 0.10382693,...|\n",
      "|  6|[0.009057933, 4.9...|\n",
      "|  7|[0.02530831, 0.0,...|\n",
      "|  8|[0.028864225, 0.0...|\n",
      "|  9|[0.0013958388, 0....|\n",
      "| 10|[0.02542325, 0.04...|\n",
      "| 11|[0.044405952, 0.1...|\n",
      "| 12|[0.030377503, 0.0...|\n",
      "| 13|[0.0, 0.02294527,...|\n",
      "| 14|[0.0, 0.0, 0.0942...|\n",
      "| 15|[0.0, 0.15974322,...|\n",
      "| 16|[0.035361394, 0.0...|\n",
      "| 17|[0.0, 0.027111717...|\n",
      "| 18|[0.030113203, 0.0...|\n",
      "| 19|[0.03618488, 0.08...|\n",
      "| 20|[0.008329038, 0.0...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userFactors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  1|[0.0, 0.051413972...|\n",
      "|  2|[0.05016716, 0.05...|\n",
      "|  3|[0.04314227, 0.04...|\n",
      "|  4|[0.0, 0.0, 0.4614...|\n",
      "|  5|[0.020771416, 0.0...|\n",
      "|  6|[0.11821658, 0.27...|\n",
      "|  7|[0.055226717, 0.0...|\n",
      "|  8|[0.059798315, 0.0...|\n",
      "|  9|[0.032114543, 0.0...|\n",
      "| 10|[0.019956274, 0.0...|\n",
      "| 11|[0.040037796, 0.0...|\n",
      "| 12|[0.030063575, 0.0...|\n",
      "| 13|[0.08434692, 0.13...|\n",
      "| 14|[0.08940532, 0.05...|\n",
      "| 15|[0.020502938, 0.1...|\n",
      "| 16|[0.020317668, 0.0...|\n",
      "| 17|[0.04921029, 0.10...|\n",
      "| 18|[0.13009888, 0.19...|\n",
      "| 19|[0.0, 0.0, 0.0624...|\n",
      "| 20|[0.0, 0.004746076...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "itemFactors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These latent factors allow us to express the sparse high-dimensional user by item by ratings space as a dense lower dimensional space (i.e., by a user factors by item factors by ratings space). Using these factors we can complete the user by item by rating matrix. Ranking previously unrated items by rating for each user then allows us to provide recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create user factor table\n",
    "userFactors.createOrReplaceTempView(\"ALS_user_factors\")\n",
    "# create item factor table\n",
    "itemFactors.createOrReplaceTempView(\"ALS_item_factors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately above, we create a temporary view of the data that can be used to facilitate data manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the top 10 movies for each user using our best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTop=10\n",
    "ALS_recommendations=best_model.recommendForAllUsers(nTop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|    471|[[102, 4.604454],...|\n",
      "|    463|[[20, 4.2555323],...|\n",
      "|    833|[[1187, 4.4711742...|\n",
      "|    496|[[56, 4.1801705],...|\n",
      "|    148|[[169, 4.9311213]...|\n",
      "|    540|[[1449, 4.774058]...|\n",
      "|    392|[[1449, 4.985334]...|\n",
      "|    243|[[1449, 4.673945]...|\n",
      "|    623|[[496, 4.440423],...|\n",
      "|    737|[[179, 4.81333], ...|\n",
      "|    897|[[313, 4.679883],...|\n",
      "|    858|[[127, 4.279135],...|\n",
      "|     31|[[1449, 4.8956256...|\n",
      "|    516|[[408, 4.73162], ...|\n",
      "|    580|[[50, 4.654353], ...|\n",
      "|    251|[[50, 4.745293], ...|\n",
      "|    451|[[313, 4.1940765]...|\n",
      "|     85|[[1463, 4.38562],...|\n",
      "|    137|[[96, 5.17765], [...|\n",
      "|    808|[[316, 5.304805],...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ALS_recommendations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create view of our recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_recommendations.createOrReplaceTempView(\"ALS_recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next several cells, we rearrange the data into a 'long' format (i.e., user ID, item ID, rating prediction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_recommendations = spark.sql(\"SELECT user_id,explode(recommendations) AS recommendation FROM ALS_recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|user_id|   recommendation|\n",
      "+-------+-----------------+\n",
      "|    471|  [102, 4.604454]|\n",
      "|    471|  [342, 4.549936]|\n",
      "|    471| [140, 4.4436836]|\n",
      "|    471| [477, 4.3312244]|\n",
      "|    471|   [82, 4.323793]|\n",
      "|    471|  [378, 4.317134]|\n",
      "|    471| [422, 4.3069406]|\n",
      "|    471| [465, 4.2862034]|\n",
      "|    471|    [8, 4.259362]|\n",
      "|    471| [1169, 4.253462]|\n",
      "|    463|  [20, 4.2555323]|\n",
      "|    463| [1449, 4.213945]|\n",
      "|    463|[1167, 4.0801134]|\n",
      "|    463| [302, 4.0746717]|\n",
      "|    463|  [408, 4.050413]|\n",
      "|    463|   [116, 4.01069]|\n",
      "|    463|  [887, 3.991781]|\n",
      "|    463| [253, 3.9703588]|\n",
      "|    463|[1137, 3.9649327]|\n",
      "|    463| [361, 3.9490182]|\n",
      "+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploded_recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "topN_clean_recommendations = spark.sql(\"SELECT user_id,item_ids_and_ratings.item_id AS item_id,item_ids_and_ratings.rating AS prediction FROM ALS_recommendations LATERAL VIEW explode(recommendations) exploded_table AS item_ids_and_ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|user_id|item_id|prediction|\n",
      "+-------+-------+----------+\n",
      "|    471|    102|  4.604454|\n",
      "|    471|    342|  4.549936|\n",
      "|    471|    140| 4.4436836|\n",
      "|    471|    477| 4.3312244|\n",
      "|    471|     82|  4.323793|\n",
      "|    471|    378|  4.317134|\n",
      "|    471|    422| 4.3069406|\n",
      "|    471|    465| 4.2862034|\n",
      "|    471|      8|  4.259362|\n",
      "|    471|   1169|  4.253462|\n",
      "|    463|     20| 4.2555323|\n",
      "|    463|   1449|  4.213945|\n",
      "|    463|   1167| 4.0801134|\n",
      "|    463|    302| 4.0746717|\n",
      "|    463|    408|  4.050413|\n",
      "|    463|    116|   4.01069|\n",
      "|    463|    887|  3.991781|\n",
      "|    463|    253| 3.9703588|\n",
      "|    463|   1137| 3.9649327|\n",
      "|    463|    361| 3.9490182|\n",
      "+-------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topN_clean_recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9430, 3)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topN_clean_recommendations.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the title to the recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------------+\n",
      "|item_id|user_id|prediction|               title|\n",
      "+-------+-------+----------+--------------------+\n",
      "|    474|    737| 4.5123734|Dr. Strangelove o...|\n",
      "|    474|     31|  4.637323|Dr. Strangelove o...|\n",
      "|    474|     85| 4.1466703|Dr. Strangelove o...|\n",
      "|    474|    321| 4.0486994|Dr. Strangelove o...|\n",
      "|    474|     76| 4.5824924|Dr. Strangelove o...|\n",
      "|    474|    916| 4.2088294|Dr. Strangelove o...|\n",
      "|    474|    409| 4.3826313|Dr. Strangelove o...|\n",
      "|    474|    822| 3.8614087|Dr. Strangelove o...|\n",
      "|    474|    601| 4.0764046|Dr. Strangelove o...|\n",
      "|    474|    875| 4.7661343|Dr. Strangelove o...|\n",
      "|    474|    686| 5.0842505|Dr. Strangelove o...|\n",
      "|    474|    232|  4.526551|Dr. Strangelove o...|\n",
      "|    474|    360|  4.588858|Dr. Strangelove o...|\n",
      "|    474|    855| 3.7666855|Dr. Strangelove o...|\n",
      "|    474|    305|  4.122358|Dr. Strangelove o...|\n",
      "|    474|    747| 4.8431907|Dr. Strangelove o...|\n",
      "|    474|    325| 4.6233015|Dr. Strangelove o...|\n",
      "|    474|    581| 4.4574666|Dr. Strangelove o...|\n",
      "|    474|    774|  2.832162|Dr. Strangelove o...|\n",
      "|    474|    704| 4.3167586|Dr. Strangelove o...|\n",
      "|    474|      6|  4.460924|Dr. Strangelove o...|\n",
      "|    474|    250|   4.67651|Dr. Strangelove o...|\n",
      "|    474|    582|  4.317267|Dr. Strangelove o...|\n",
      "|    474|     94|  4.818317|Dr. Strangelove o...|\n",
      "|    474|    570| 3.3452294|Dr. Strangelove o...|\n",
      "|    474|    306| 4.8842325|Dr. Strangelove o...|\n",
      "|    474|    339| 5.0029545|Dr. Strangelove o...|\n",
      "|    474|    235|  4.506578|Dr. Strangelove o...|\n",
      "|    474|    840|  4.847913|Dr. Strangelove o...|\n",
      "|    474|    268| 4.0652137|Dr. Strangelove o...|\n",
      "|    474|    397| 4.7936373|Dr. Strangelove o...|\n",
      "|    474|    641|  4.771748|Dr. Strangelove o...|\n",
      "|    474|    299|  4.107348|Dr. Strangelove o...|\n",
      "|    474|     41| 4.5386863|Dr. Strangelove o...|\n",
      "|    474|    154| 4.6352687|Dr. Strangelove o...|\n",
      "|    474|    607| 4.2353134|Dr. Strangelove o...|\n",
      "|    474|    269| 4.3231215|Dr. Strangelove o...|\n",
      "|    474|    666|  4.273668|Dr. Strangelove o...|\n",
      "|    474|    387|  4.416481|Dr. Strangelove o...|\n",
      "|    474|    381|  4.786061|Dr. Strangelove o...|\n",
      "|    474|    498| 3.9603007|Dr. Strangelove o...|\n",
      "|    474|    583| 4.9433136|Dr. Strangelove o...|\n",
      "|    474|    456|  4.200866|Dr. Strangelove o...|\n",
      "|    474|    884| 4.4650106|Dr. Strangelove o...|\n",
      "|    474|    766|  4.035426|Dr. Strangelove o...|\n",
      "|    474|    537| 3.7433317|Dr. Strangelove o...|\n",
      "|    474|    890|  4.565643|Dr. Strangelove o...|\n",
      "|    474|    114|   4.16738|Dr. Strangelove o...|\n",
      "|    474|    669|  4.213088|Dr. Strangelove o...|\n",
      "|    474|     23| 4.3558702|Dr. Strangelove o...|\n",
      "|    474|    380| 3.7641752|Dr. Strangelove o...|\n",
      "|    474|    736| 4.0278106|Dr. Strangelove o...|\n",
      "|    474|     49|   4.04437|Dr. Strangelove o...|\n",
      "|    474|    136|   4.70588|Dr. Strangelove o...|\n",
      "|    474|    664|  4.503182|Dr. Strangelove o...|\n",
      "|    474|    794|  4.978548|Dr. Strangelove o...|\n",
      "|    474|    391| 4.4489994|Dr. Strangelove o...|\n",
      "|    474|    234| 3.8524477|Dr. Strangelove o...|\n",
      "|    474|    264| 4.9566903|Dr. Strangelove o...|\n",
      "|    474|    389|  4.463228|Dr. Strangelove o...|\n",
      "|    474|     77|  4.325265|Dr. Strangelove o...|\n",
      "|    474|    293|  4.081218|Dr. Strangelove o...|\n",
      "|    474|    249| 4.8527775|Dr. Strangelove o...|\n",
      "|    474|     73|  4.580916|Dr. Strangelove o...|\n",
      "|    474|    763| 4.3986816|Dr. Strangelove o...|\n",
      "|    474|    237| 4.4475794|Dr. Strangelove o...|\n",
      "|    474|    645| 4.7107058|Dr. Strangelove o...|\n",
      "|    474|    480| 4.0594754|Dr. Strangelove o...|\n",
      "|    474|    156| 4.1127324|Dr. Strangelove o...|\n",
      "|    474|    526|  4.646658|Dr. Strangelove o...|\n",
      "|    474|    492| 3.9776974|Dr. Strangelove o...|\n",
      "|    474|    933| 3.7772129|Dr. Strangelove o...|\n",
      "|    474|    595|  4.368574|Dr. Strangelove o...|\n",
      "|    474|    767|  4.770065|Dr. Strangelove o...|\n",
      "|    474|     60| 4.5824413|Dr. Strangelove o...|\n",
      "|    474|    151| 4.7961183|Dr. Strangelove o...|\n",
      "|    474|    508|  4.563019|Dr. Strangelove o...|\n",
      "|    474|     71| 4.3281612|Dr. Strangelove o...|\n",
      "|    474|    567| 4.5214944|Dr. Strangelove o...|\n",
      "|    474|    561| 3.9926393|Dr. Strangelove o...|\n",
      "|    474|    805|  4.471777|Dr. Strangelove o...|\n",
      "|    474|    198|  4.033653|Dr. Strangelove o...|\n",
      "|    474|    344| 4.4956455|Dr. Strangelove o...|\n",
      "|    474|    123|  4.550036|Dr. Strangelove o...|\n",
      "|    474|    199| 4.3482757|Dr. Strangelove o...|\n",
      "|    474|    522|  4.749099|Dr. Strangelove o...|\n",
      "|    474|     79| 4.7896414|Dr. Strangelove o...|\n",
      "|    474|    773|   4.28713|Dr. Strangelove o...|\n",
      "|    474|    571| 3.7390826|Dr. Strangelove o...|\n",
      "|    474|    313| 4.3592634|Dr. Strangelove o...|\n",
      "|    474|    180| 4.5576086|Dr. Strangelove o...|\n",
      "|    474|    932| 4.8026414|Dr. Strangelove o...|\n",
      "|    474|    643| 4.4761868|Dr. Strangelove o...|\n",
      "|    474|    878| 4.0050316|Dr. Strangelove o...|\n",
      "|    191|    128|  4.382919|      Amadeus (1984)|\n",
      "|    191|    444| 4.5348225|      Amadeus (1984)|\n",
      "|    191|    747| 4.8446026|      Amadeus (1984)|\n",
      "|    191|    295|  4.863581|      Amadeus (1984)|\n",
      "|    191|    292|  4.731183|      Amadeus (1984)|\n",
      "|    191|     48|  4.230653|      Amadeus (1984)|\n",
      "+-------+-------+----------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "item_info=sqlContext.sql('select distinct item_id,title from movielens order by item_id,title')\n",
    "#\n",
    "ratings=sqlContext.sql('select * from movielens order by item_id,title,user_id')\n",
    "#\n",
    "topN_recommendations=topN_clean_recommendations.join(item_info, [\"item_id\"], \"left\")\n",
    "#\n",
    "topN_recommendations.show(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the actual rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------------+------+----------+\n",
      "|user_id|item_id|prediction|               title|rating|titmestamp|\n",
      "+-------+-------+----------+--------------------+------+----------+\n",
      "|      9|    172|   4.76347|                null|  null|      null|\n",
      "|     15|    754| 3.8676977|   Red Corner (1997)|     5| 879455080|\n",
      "|     25|     12|  4.458394|                null|  null|      null|\n",
      "|     51|    172| 4.0368223|Empire Strikes Ba...|     5| 883498936|\n",
      "|     56|     96| 4.5499263|Terminator 2: Jud...|     5| 892676429|\n",
      "|     61|    169|  3.659671|                null|  null|      null|\n",
      "|     79|   1159| 4.8410177|                null|  null|      null|\n",
      "|     82|   1463| 3.9947956|                null|  null|      null|\n",
      "|     96|    169| 4.7422895|                null|  null|      null|\n",
      "|     99|    318| 4.3608117|                null|  null|      null|\n",
      "|    114|    100| 3.9224412|        Fargo (1996)|     5| 881259927|\n",
      "|    116|    127| 3.8134534|Godfather, The (1...|     5| 876454257|\n",
      "|    123|   1449|   4.77765|                null|  null|      null|\n",
      "|    127|    323| 4.7998834|                null|  null|      null|\n",
      "|    176|    251| 4.3905926|                null|  null|      null|\n",
      "|    199|    137| 4.3044457|                null|  null|      null|\n",
      "|    207|    496| 3.9130802|                null|  null|      null|\n",
      "|    216|    113|  4.586956|                null|  null|      null|\n",
      "|    217|     22| 3.8277073|   Braveheart (1995)|     5| 889069741|\n",
      "|    259|    169|  4.582014|                null|  null|      null|\n",
      "+-------+-------+----------+--------------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topN_clean_recommendations.join(ratings, [\"user_id\", \"item_id\"], \"left\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we filter all of the originally unrated items for which we have generated predicted ratings using our best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+-----+------+----------+\n",
      "|user_id|item_id|prediction|title|rating|titmestamp|\n",
      "+-------+-------+----------+-----+------+----------+\n",
      "|      7|    963| 4.7649426| null|  null|      null|\n",
      "|     23|    302| 4.3101945| null|  null|      null|\n",
      "|     25|     12|  4.448552| null|  null|      null|\n",
      "|     79|   1159|  4.856906| null|  null|      null|\n",
      "|     81|     56| 4.1670837| null|  null|      null|\n",
      "|     82|   1463| 3.9235337| null|  null|      null|\n",
      "|     96|    169| 4.7215858| null|  null|      null|\n",
      "|    123|   1449| 4.8516417| null|  null|      null|\n",
      "|    170|   1192| 4.4448237| null|  null|      null|\n",
      "|    199|    137| 4.4266148| null|  null|      null|\n",
      "|    207|    496|  3.945725| null|  null|      null|\n",
      "|    216|    113|  4.562116| null|  null|      null|\n",
      "|    238|   1169|   4.04074| null|  null|      null|\n",
      "|    259|    169| 4.6149435| null|  null|      null|\n",
      "|    310|    408|   4.97844| null|  null|      null|\n",
      "|    341|    603| 4.3532605| null|  null|      null|\n",
      "|    386|    144| 4.0065784| null|  null|      null|\n",
      "|    408|   1368| 4.7086077| null|  null|      null|\n",
      "|    420|   1193| 4.6553245| null|  null|      null|\n",
      "|    428|   1368| 4.5945926| null|  null|      null|\n",
      "+-------+-------+----------+-----+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "unrated_recommendations=clean_recommendations.join(ratings, [\"user_id\", \"item_id\"], \n",
    "    \"left\").filter(ratings.rating.isNull())\n",
    "#\n",
    "unrated_recommendations.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5440, 6)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_recommendations.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retreive sample recommendations for user 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "topN_clean_recommendations_user_5 = spark.sql(\"SELECT user_id,item_ids_and_ratings.item_id AS item_id,item_ids_and_ratings.rating AS prediction FROM ALS_recommendations LATERAL VIEW explode(recommendations) exploded_table AS item_ids_and_ratings WHERE user_id=5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------------------+\n",
      "|item_id|user_id|prediction|               title|\n",
      "+-------+-------+----------+--------------------+\n",
      "|     50|      5| 4.2129393|    Star Wars (1977)|\n",
      "|    114|      5| 4.3055744|Wallace & Gromit:...|\n",
      "|     89|      5| 4.1861587| Blade Runner (1982)|\n",
      "|   1449|      5|  4.152075|Pather Panchali (...|\n",
      "|    390|      5| 4.3298526|Fear of a Black H...|\n",
      "|    175|      5| 4.1830025|       Brazil (1985)|\n",
      "|    408|      5|  4.525527|Close Shave, A (1...|\n",
      "|   1500|      5| 4.1414766|Santa with Muscle...|\n",
      "|    189|      5|  4.179242|Grand Day Out, A ...|\n",
      "|    169|      5| 4.5346627|Wrong Trousers, T...|\n",
      "+-------+-------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topN_clean_recommendations_user_5.join(item_info, [\"item_id\"], \"left\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a couple of the recommendations are the same for both the SGD and ALS models, namely 'A Close Shave' and 'Star Wars'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "In this section, we provide a quick summary comparison of the performance of our original SVD-like recommender system (created for Project 3), with our more scalable SVD-like matrix factorization implemented using PySpark. Although the RMSE for the ALS model was 0.9097 versus 0.9486 for the SGD model. Although the random seed was the same for the cross-validation, the parameter grid used to search for 'best' parameters was necessarily different. We were not able to run as many iterations for the PySpark implementation.\n",
    "\n",
    "The PySpark implementation was able to utilize all 18 CPU cores on our workstation and was much faster than the 'surprise' implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we were able to reduce the scope of the PySpark implementation by using a docker container where both the Python and Spark setup were addressed almost entirely for us, setting up a real cluster in order to scale out our recommender system is complex. This complexity is only warranted once the size of the dataset moves beyond the amount of memory that can cost-effectively be put in a single server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although the predictive performance of both recommender systems was reasonably good, as our dataset size expands beyond what we can fit into memory on a single server, it becomes necessary to modify our approach to use a distributed computing environment. Our PySpark implementation allows us to scale beyond the memory size of a single server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Golub, G. H. and Van Loan, C. F. Matrix Computations, 3rd ed. Baltimore, MD: Johns Hopkins, 1996. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
